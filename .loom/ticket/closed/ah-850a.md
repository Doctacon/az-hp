---
"id": "ah-850a"
"status": "closed"
"deps":
- "ah-2fa8"
"links": []
"created": "2026-02-18T02:07:11Z"
"type": "task"
"priority": 2
"assignee": "Connor"
"parent": "ah-174b"
"tags":
- "sprint:map-display-fix"
"external": {}
---
# Investigate and optimize data pipeline performance

# Investigate and optimize data pipeline performance

## Objective Alignment
The data pipeline takes over 15 minutes. This ticket investigates bottlenecks and implements optimizations to reduce runtime.

## Scope
- Measure timing of each pipeline step
- Identify slowest operations (likely: Overture downloads, spatial joins)
- Implement targeted optimizations
- Document remaining bottlenecks and future optimization paths

## Non-goals
- Complete rewrite of pipeline architecture
- Adding caching layers (unless simple)
- Parallelizing downloads (complex change)

## Implementation Plan

1. **Instrument the pipeline with timing**:
   - Add timing logs to each step in `01_download.py`, `02_clip_arizona.py`, `03_enrich.py`, `04_generate_tiles.sh`
   - Example instrumentation:
   ```python
   import time
   start = time.time()
   # ... operation ...
   print(f"  Completed in {time.time() - start:.1f}s")
   ```

2. **Run pipeline and capture timing data**:
   ```bash
   time make all 2>&1 | tee pipeline_timing.log
   ```

3. **Analyze bottlenecks** (likely candidates):
   - **Overture downloads**: S3 queries over large datasets
     - Optimization: Verify bbox filtering is working; consider smaller bbox tiles
   - **Spatial joins** in `03_enrich.py`: 
     - `roads x land ownership` - O(n*m) operation
     - `roads x hunt units` - O(n*m) operation
     - Optimization: Use spatial index (RTree), reduce precision, or sample
   - **GeoJSON conversion**: Large files may be slow
     - Optimization: Skip unnecessary columns, simplify geometries

4. **Quick wins to implement**:
   - Add spatial index before sjoin: `sma.sindex` (geopandas does this automatically, but verify)
   - Reduce geometry precision for spatial joins: `gdf.geometry = gdf.geometry.simplify(0.001)`
   - Parallel downloads if time allows (use ThreadPoolExecutor)

5. **Document remaining bottlenecks** for future work

## Verification Commands
```bash
# Time the full pipeline
time make all

# After optimization, verify improvement
time make all

# Check specific step timing
time uv run python pipeline/03_enrich.py
```

## Acceptance Criteria
- [ ] Each pipeline step has timing logs
- [ ] Pipeline completes in under 10 minutes (target: 50% reduction from 15+ min)
- [ ] Top 3 bottlenecks documented with specific timing data
- [ ] At least one optimization implemented

## Risks/Edge Cases
- **Network variability**: Download times may vary - run multiple times
- **Memory pressure**: Large datasets may cause swapping - monitor RAM
- **Incomplete optimization**: May only achieve partial improvement

## Dependencies
- Requires: ah-2fa8 (Run full data pipeline) - needs to run pipeline first to measure
- Can run in parallel with: ah-d095, ah-ed69 (different worktrees)

## Acceptance Criteria

Pipeline completes in under 10 minutes; bottlenecks documented with specific timing data
